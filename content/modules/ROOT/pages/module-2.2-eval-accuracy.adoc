:imagesdir: ../assets/images
[#eval-accuracy]
# Evaluating Model Accuracy with Trusty AI lm-eval-harness service

While performance metrics like latency and throughput are critical for deploying efficient GenAI systems, **task-level accuracy and reasoning quality** are equally essential for selecting or fine-tuning a model. In this activity, we use the popular lm-eval-harness framework to evaluate how well a language model performs across established benchmarks, focusing on reasoning and subject matter understanding.

## What is Trusty AI?

https://trustyai.org/docs/main/main

## What is lm-eval-harness?

**lm-eval-harness** is a community-maintained benchmarking toolkit from **EleutherAI**. It enables consistent, reproducible evaluation of large language models (LLMs) across dozens of academic and real-world benchmarks, such as:

* MMLU (Massive Multitask Language Understanding)

* HellaSwag, ARC, and Winogrande

* Question answering, common sense reasoning, reading comprehension, and more

The framework supports both open-source models and OpenAI-compatible endpoints, and can be customized with additional tasks, prompt templates, and evaluation metrics.

## MMLU-Pro

Today we will be running the mmlu_pro evaluation. 

**MMLU-Pro** is a reasoning-focused, multiple-choice benchmark derived from the original MMLU dataset. MMLU-Pro extends the original MMLU benchmark by introducing 10-option multiple-choice questions across diverse academic disciplines. It’s designed to test a model’s **reasoning, factual recall, and elimination skills**—key for enterprise AI.

## Today's Activity

In this section of our lab we will:

. Set up the Trusty AI operator
. Create and run the lm-eval job
. Interpret and understand results

### Setup Trusty AI

* Go to the DataScienceCluster resource instance.

* Change the trustyai managementState from "Removed" to "Managed"
https://github.com/trustyai-explainability/llama-stack-provider-lmeval/blob/main/demos/00-getting_started_with_lmeval.ipynb
https://trustyai.org/docs/main/lm-eval-tutorial#_examples

#### 1. Configure TrustyAI to allow downloading remote datasets from Huggingface
By default, TrustyAI prevents evaluation jobs from accessing the internet or running downloaded code.
A typical evaluation job will download two items from Huggingface:
1) The dataset of the evaluation task, and any dataset processing code
2) The tokenizer of your model

If you trust the source of your dataset and tokenizer, you can override TrustyAI's default setting.
In our case, we'll be downloading:
1) [allenai/ai2_arc](https://huggingface.co/datasets/allenai/ai2_arc)
2) [Phi-3-mini-4k-instruct's tokenizer](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)

If you are happy for TrustyAI to automatically download those two resources, run:

[source,console,role=execute,subs=attributes+]
----
oc patch configmap trustyai-service-operator-config -n redhat-ods-applications  \
--type merge -p '{"metadata": {"annotations": {"opendatahub.io/managed": "false"}}}'
oc patch configmap trustyai-service-operator-config -n redhat-ods-applications \
--type merge -p '{"data":{"lmes-allow-online":"true","lmes-allow-code-execution":"true"}}'
oc rollout restart deployment trustyai-service-operator-controller-manager -n redhat-ods-applications
----
Wait for your `trustyai-service-operator-controller-manager` pod in the `redhat-ods-applications` namespace
to restart, and then TrustyAI should be ready to go.

#### 2. Ensure Granite model is configured

Add the external endpoint to the evaluation_job.yaml file in the following section:

[source,console,role=execute,subs=attributes+]
----
- name: base_url
      value: https://granite-8b-instruct-vllm-kserve-model-namespace.apps.ocp.z9rfv.sandbox447.opentlc.com/v1/completions # the location of your model's /chat/completions or /completions endpoint
----

#### 4. Run the evaluation

To start an evaluation, apply an `LMEvalJob` custom resource as defined in the following file:

[source,console,role=execute,subs=attributes+]
----
oc apply -f workshop_code/evals/trusty/arc_easy.yaml
----

Check out the evaluation_job.yaml file to learn more about the `LMEvalJob` specification.

If everything has worked, you should see a pod called `arc-easy-eval-job` running in your namespace. 
You can watch the progress of your evaluation job by running:

[source,console,role=execute,subs=attributes+]
----
oc logs -f arc-easy-eval-job
----
You will see progression in percentage points.

or alternatively the logs of the model pod:

[source,console,role=execute,subs=attributes+]
----
oc logs -f granite-8b-instruct-vllm-kserve-predictor-<exact-pod-name>
----

You will see the exact questions getting passed to the model endpoint.

This evaluation run will take approximately 10 minutes.

#### 5. While You Wait: What is lm-eval-harness?

* Check out this https://github.com/EleutherAI/lm-evaluation-harness/blob/main/examples/lm-eval-overview.ipynb[overview notebook] to explore extensibility and task definitions.

* View real Red Hat https://huggingface.co/collections/RedHatAI/red-hat-ai-validated-models-v10-682613dc19c4a596dbac9437[validated model results] to understand benchmark outcomes in production contexts and to see how your favorite models rank.

##### Interpreting MMLU-Pro Results

**Accuracy**: The primary metric is multiple-choice accuracy, indicating how often the model selects the correct answer from 10 options.

* ~10% = random guessing baseline

* ~30–50% = typical for smaller or untuned models

* ~60–70%+ = high reasoning capability or fine-tuned performance

**Per-subject Scores**: Breakdowns by subject (e.g., philosophy, law, computer science) help identify a model’s strengths and weaknesses in specific domains.

**Implications**: Higher MMLU-Pro accuracy generally correlates with better real-world task generalization, especially for tasks involving structured inputs, knowledge retrieval, and logic.


#### 6. Check out the results

After the evaluation finishes (it took about 8.5 minutes on my cluster), you can take a look at the results. These are stored in the `status.results` field of the LMEvalJob resource:

[source,console,role=execute,subs=attributes+]
----
oc get LMEvalJob arc-easy-eval-job -o template --template '{{.status.results}}' | jq  .results
----

returns:
[source,console]
----
{
  "arc_easy": {
    "alias": "arc_easy",
    "acc,none": 0.8186026936026936,
    "acc_stderr,none": 0.007907153952801706,
    "acc_norm,none": 0.7836700336700336,
    "acc_norm_stderr,none": 0.00844876352205705
  }
}
----

Now you're free to play around with evaluations! You can see the full list of evaluation supported by 
lm-evaluation-harness [here.](https://github.com/red-hat-data-services/lm-evaluation-harness/blob/main/lm_eval/tasks/README.md)
## More information
- [TrustyAI Notes Repo](https://github.com/trustyai-explainability/reference/tree/main)
- [TrustyAI Github](https://github.com/trustyai-explainability)

#### 7. Try MMLU industry-focused test

In some cases, you may want to check that a model has retained accuracy around a standard, specific dataset topic. 

Let's try the mmlu_jurisprudence dataset to test the model's knowledge on law.

[source,console,role=execute,subs=attributes+]
----
oc apply -f workshop_code/evals/trusty/mmlu_jurisprudence.yaml
----

This will only take a minute or so to process. 

[source,console,role=execute,subs=attributes+]
----
oc get LMEvalJob mmlu-jurisprudence-eval-job -o template --template '{{.status.results}}' | jq  .results
----

// TODO:### Testing your model with a custom dataset 

// TODO:### Testing your model in a disconnected environment

## Summary

What We Did:
* Set up TrustyAI operator - Enabled model evaluation framework in OpenShift AI
* Configured internet access - Allowed downloading of evaluation datasets from HuggingFace
* Connected to deployed model - Linked evaluation job to the Granite 8B inference service
* Ran ARC Easy benchmark - Tested model's reasoning on grade-school science questions
* Analyzed results - Achieved 81.8% accuracy, indicating strong reasoning performance

Key Outcome:
* ✅ Successfully evaluated deployed AI model accuracy using industry-standard benchmarks through TrustyAI + lm-eval-harness

Tools Used:
* TrustyAI: Enterprise evaluation operator
* lm-eval-harness: Standard benchmarking framework
* ARC Easy: Science reasoning benchmark
* Bottom Line: Demonstrated how to measure and validate AI model accuracy in production using automated evaluation pipelines.

